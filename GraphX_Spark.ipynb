{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>GraphX Assignment</h1>\n",
    "In this assignment, you need to do the following:\n",
    "<li>Read the file 201710-citibike-tripdata.csv</li>\n",
    "<li>Construct a graph with stations as vertices and trips between stations as edges</li>\n",
    "<li>Vertex Ids are station numbers and Vertex attributes are station names</li>\n",
    "<li>Edge attributes are trip duration (the first field in the file - durations are in seconds)</li>\n",
    "<li>Then answer the questions below</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.14:4041\n",
       "SparkContext available as 'sc' (version = 2.3.3, master = local[*], app id = local-1555991114350)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.graphx._\r\n",
       "import org.apache.spark.rdd.RDD\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text: org.apache.spark.rdd.RDD[String] = C://Users//vaish//Downloads//201710-citibike-tripdata (1).csv MapPartitionsRDD[1] at textFile at <console>:29\r\n",
       "text_header_removed: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at mapPartitionsWithIndex at <console>:30\r\n",
       "vertexRDD: org.apache.spark.rdd.RDD[(Long, String)] = UnionRDD[5] at union at <console>:32\r\n",
       "edgeRDD: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Int]] = MapPartitionsRDD[6] at map at <console>:33\r\n",
       "graph: org.apache.spark.graphx.Graph[String,Int] = org.apache.spark.graphx.impl.GraphImpl@d2e9567\r\n",
       "res0: org.apache.spark.graphx.Graph[String,Int] = org.apache.spark.graphx.impl.GraphImpl@23b6274d\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val text = sc.textFile(\"C://Users//vaish//Downloads//201710-citibike-tripdata (1).csv\")\n",
    "val text_header_removed = text.mapPartitionsWithIndex{ (idx,iter) => if (idx==0) iter.drop(1) else iter}\n",
    "val vertexRDD = (text_header_removed.map(l => ((l.split(\",\")(3).toLong,(l.split(\",\")(4))))).\n",
    "                 union(text_header_removed.map(l => ((l.split(\",\")(7).toLong,(l.split(\",\")(8)))))))\n",
    "val edgeRDD = text_header_removed.map(l=> (Edge(l.split(\",\")(3).toLong, l.split(\",\")(7).toLong, l.split(\",\")(0).toInt)))\n",
    "val graph: Graph[(String), Int] = Graph(vertexRDD, edgeRDD)\n",
    "graph.partitionBy(PartitionStrategy.CanonicalRandomVertexCut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Station from which most trips originate</h2>\n",
    "<li>Print the name of the station from which most trips originate</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "most: String = \"Pershing Square North\"\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val most = graph.outDegrees.join(vertexRDD).distinct.sortBy(_._2._1, ascending=false).first._2._2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The station with the most originating trips is \"Pershing Square North\"\n"
     ]
    }
   ],
   "source": [
    "println(s\"The station with the most originating trips is ${most}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Proportion of trips for each station that start and end at that same station</h2>\n",
    "<li>Use aggregateMessages to calculate the number of trips that start and end at the same vertex (for each vertex)</li>\n",
    "<li>Divide that number by the number of trips that originate at that vertex using a join with outDegrees</li>\n",
    "<li>Note that some stations may have no trips at all (i.e., the denominator may be non-existent)</li>\n",
    "<li>Print the names of the top ten stations with the highest proportions</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "start_end_agg: org.apache.spark.graphx.VertexRDD[Int] = VertexRDDImpl[50] at RDD at VertexRDD.scala:57\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val start_end_agg = graph.aggregateMessages[Int](ec => if (ec.srcId == ec.dstId) {ec.sendToSrc(1)}, (x,y) => x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prop_map: org.apache.spark.rdd.RDD[(Long, Double)] = MapPartitionsRDD[54] at map at <console>:37\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val prop_map = start_end_agg.fullOuterJoin(graph.outDegrees).map(t => (t._1.toLong, t._2 match {\n",
    "    case ((Some(b), Some(c))) => (b.toDouble/c.toDouble)\n",
    "    case ((Some(b), None)) => 0\n",
    "    case ((None, Some(c))) => 0\n",
    "    case ((None, None)) => 0\n",
    "}))                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: (Long, Double) = (3454,0.014084507042253521)\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop_map.map(t => ((t._1), t._2)).first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Names of stations with highest proportions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"8D QC Station 01\" - 1.0\n",
      "\"NYCBS DEPOT - DELANCEY\" - 0.5\n",
      "\"Yankee Ferry Terminal\" - 0.3433333333333333\n",
      "\"Soissons Landing\" - 0.32545454545454544\n",
      "\"Pioneer St & Richards St\" - 0.19732441471571907\n",
      "\"39 St & 2 Ave - Citi Bike HQ at Industry City\" - 0.1836734693877551\n",
      "\"Ditmars Blvd & 19 St\" - 0.171990171990172\n",
      "\"Brooklyn Bridge Park - Pier 2\" - 0.17134416543574593\n",
      "\"West Drive & Prospect Park West\" - 0.14900527811611855\n",
      "\"Expansion Warehouse 333 Johnson Ave\" - 0.125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "top10: Unit = ()\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val top10 = vertexRDD.join(prop_map).distinct.sortBy(_._2._2, ascending=false).map(t =>\n",
    "    t._2._1 + \" - \" + t._2._2).take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create a new graph that contains all edges except for those between the same station</h2>\n",
    "<li>Report the number of edges in the new graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "new_graph: org.apache.spark.graphx.Graph[String,Int] = org.apache.spark.graphx.impl.GraphImpl@6891e514\r\n",
       "res3: Long = 1864347\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//val new_edge_RDD = graph.edges.filter(t => t.srcId != t.dstId)\n",
    "//val new_graph = (vertexRDD, new_edge_RDD)\n",
    "val new_graph: Graph[(String), Int] = Graph(vertexRDD, graph.edges.filter(t => t.srcId != t.dstId))\n",
    "new_graph.edges.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Calculate the average time between every pair of stations</h2>\n",
    "<li>Use groupEdges to calculate the total time between each pair of stations</li>\n",
    "<li>Get rdd of count of number of edges by using reduceByKey on graph.edges</li>\n",
    "<li>get edges from groupEdges in the same pair format as the count of number of edges</li>\n",
    "<li>join the two rdds, use map to calculate averages, construct new edges</li>\n",
    "<li>make a new graph with vertices from total_times and the new edges you've just constructed</li>\n",
    "<li>Print the top 10 edges with the highest average time between stations<li>\n",
    "    \n",
    "Sample Output:\n",
    "<pre>\n",
    "From: \"Fulton St & Utica Ave\"\tTo: \"Crown St & Bedford Ave\"\tAverage Time: 9340261.0\n",
    "From: \"Verona Pl & Fulton St\"\tTo: \"Humboldt St & Varet St\"\tAverage Time: 9163224.0\n",
    "From: \"Albany Ave & Fulton St\"\tTo: \"Sullivan Pl & Bedford Ave\"\tAverage Time: 8461569.0\n",
    "From: \"S Portland Ave & Hanson Pl\"\tTo: \"NYCBS DEPOT - DELANCEY\"\tAverage Time: 5967124.0\n",
    "From: \"Greenwich Ave & Charles St\"\tTo: \"NYCBS Depot - GOW\"\tAverage Time: 5954675.0\n",
    "From: \"Myrtle Ave & Marcy Ave\"\tTo: \"NYCBS Depot - GOW\"\tAverage Time: 5759985.0\n",
    "From: \"Macon St & Nostrand Ave\"\tTo: \"NYCBS Depot - GOW\"\tAverage Time: 5296261.5\n",
    "From: \"Lewis Ave & Madison St\"\tTo: \"NYCBS DEPOT - DELANCEY\"\tAverage Time: 4836816.0\n",
    "From: \"Albany Ave & Fulton St\"\tTo: \"NYCBS DEPOT - DELANCEY\"\tAverage Time: 4316721.0\n",
    "From: \"Union Ave & Wallabout St\"\tTo: \"NYCBS Depot - GOW\"\tAverage Time: 4218267.0\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "total_time: org.apache.spark.rdd.RDD[((Long, Long), Int)] = MapPartitionsRDD[103] at map at <console>:37\r\n",
       "total_trips: org.apache.spark.rdd.RDD[((Long, Long), Int)] = ShuffledRDD[105] at reduceByKey at <console>:39\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val total_time = graph.partitionBy(PartitionStrategy.CanonicalRandomVertexCut)\n",
    ".groupEdges((edge1, edge2) => edge1 + edge2).triplets.sortBy(_.attr, ascending=false)\n",
    ".map(triplet =>((triplet.srcId.toLong, triplet.dstId.toLong), triplet.attr))\n",
    "\n",
    "val total_trips = graph.edges.map(a => ((a.srcId.toLong, a.dstId.toLong),1)).reduceByKey((a,b)=> a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avg_time_edges: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Double]] = MapPartitionsRDD[109] at map at <console>:38\r\n",
       "avg_time_graph: org.apache.spark.graphx.Graph[String,Double] = org.apache.spark.graphx.impl.GraphImpl@3e25ca72\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val avg_time_edges = total_time.join(total_trips).map(t => Edge(t._1._1, t._1._2, (t._2._1/t._2._2).toDouble))\n",
    "val avg_time_graph: Graph[String, Double] = Graph(vertexRDD, avg_time_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: \"Fulton St & Utica Ave\" To \"Crown St & Bedford Ave\" Average Time: 9340261.0\n",
      "From: \"Verona Pl & Fulton St\" To \"Humboldt St & Varet St\" Average Time: 9163224.0\n",
      "From: \"Albany Ave & Fulton St\" To \"Sullivan Pl & Bedford Ave\" Average Time: 8461569.0\n",
      "From: \"S Portland Ave & Hanson Pl\" To \"NYCBS DEPOT - DELANCEY\" Average Time: 5967124.0\n",
      "From: \"Greenwich Ave & Charles St\" To \"NYCBS Depot - GOW\" Average Time: 5954675.0\n",
      "From: \"Myrtle Ave & Marcy Ave\" To \"NYCBS Depot - GOW\" Average Time: 5759985.0\n",
      "From: \"Macon St & Nostrand Ave\" To \"NYCBS Depot - GOW\" Average Time: 5296261.0\n",
      "From: \"Lewis Ave & Madison St\" To \"NYCBS DEPOT - DELANCEY\" Average Time: 4836816.0\n",
      "From: \"Albany Ave & Fulton St\" To \"NYCBS DEPOT - DELANCEY\" Average Time: 4316721.0\n",
      "From: \"Union Ave & Wallabout St\" To \"NYCBS Depot - GOW\" Average Time: 4218267.0\n"
     ]
    }
   ],
   "source": [
    "avg_time_graph.triplets.sortBy(_.attr, ascending=false)\n",
    "  .map(triplet =>\n",
    "    \"From: \" + triplet.srcAttr + \" To \" + triplet.dstAttr + \" Average Time: \" + triplet.attr).take(10).\n",
    "foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Important stations</h2>\n",
    "Citibike wants to figure out how best to deploy its workers in checking whether a station is over-full (too many bikes) or needs more bikes. It figures that the best way to do this is to find out which stations are the most important in terms of flows:\n",
    "<li>A station that has high bike returns and is connected to other stations with high bike returns is more likely to have too many bikes in its station and therefore should be monitored more often</li>\n",
    "<li>A station that has high bike pickups and is connected to other stations with high bike pickups is more likely to be short of bikes and therefore should be monitored more often</li>\n",
    "<li>Calculate the propensities for over-fullness and emptiness for every station</li>\n",
    "<li>Report the 10 most important stations for over-fullness</li>\n",
    "<li>Report the 10 most important stations for emptiness</li>\n",
    "\n",
    "Sample inflow importance:\n",
    "\n",
    "<pre>\n",
    "Station: \"Pershing Square North\"\tImportance: 4.930887390071603\n",
    "Station: \"West St & Chambers St\"\tImportance: 3.7410934274030576\n",
    "Station: \"Broadway & E 22 St\"\tImportance: 3.58520147183096\n",
    "Station: \"E 17 St & Broadway\"\tImportance: 3.537658018512581\n",
    "Station: \"W 21 St & 6 Ave\"\tImportance: 3.438585855241344\n",
    "Station: \"8 Ave & W 31 St\"\tImportance: 3.338364421207131\n",
    "Station: \"Centre St & Chambers St\"\tImportance: 3.15384598074596\n",
    "Station: \"Broadway & E 14 St\"\tImportance: 3.0325702821942904\n",
    "Station: \"12 Ave & W 40 St\"\tImportance: 2.9542359558471127\n",
    "Station: \"W 20 St & 11 Ave\"\tImportance: 2.8726816010849965\n",
    "</pre>\n",
    "\n",
    "Sample outflow importance:\n",
    "\n",
    "<pre>\n",
    "Station: \"Hs Don't Use\"\tImportance: 5.7106408695207485\n",
    "Station: \"Pershing Square North\"\tImportance: 5.012823444592197\n",
    "Station: \"WS Don't Use\"\tImportance: 4.272284643284594\n",
    "Station: \"Broadway & E 22 St\"\tImportance: 3.451521106903819\n",
    "Station: \"E 17 St & Broadway\"\tImportance: 3.334725974545745\n",
    "Station: \"W 21 St & 6 Ave\"\tImportance: 3.2837275306670706\n",
    "Station: \"Grand Army Plaza & Central Park S\"\tImportance: 3.269839753653885\n",
    "Station: \"West St & Chambers St\"\tImportance: 3.2398990476596206\n",
    "Station: \"8 Ave & W 31 St\"\tImportance: 3.2023851500938387\n",
    "Station: \"Central Park S & 6 Ave\"\tImportance: 3.048749328362056\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a: org.apache.spark.graphx.VertexRDD[Double] = VertexRDDImpl[2712] at RDD at VertexRDD.scala:57\r\n",
       "rank: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, Double)] = ParallelCollectionRDD[2724] at parallelize at <console>:43\r\n",
       "station: org.apache.spark.graphx.VertexRDD[String] = VertexRDDImpl[16] at RDD at VertexRDD.scala:57\r\n",
       "inTop10: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[2728] at map at <console>:45\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val a = graph.pageRank(0.0001).vertices\n",
    "val rank = sc.parallelize(a.sortBy(_._2,ascending=false).take(10))\n",
    "val station = graph.vertices\n",
    "val inTop10 = station.join(rank).map{\n",
    "    case (id,(name,r)) => (name,r)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station: \"Pershing Square North\"  Importance: 4.930887390071603   \n",
      "Station: \"West St & Chambers St\"  Importance: 3.7410934274030576   \n",
      "Station: \"Broadway & E 22 St\"  Importance: 3.58520147183096   \n",
      "Station: \"E 17 St & Broadway\"  Importance: 3.537658018512581   \n",
      "Station: \"W 21 St & 6 Ave\"  Importance: 3.438585855241344   \n",
      "Station: \"8 Ave & W 31 St\"  Importance: 3.338364421207131   \n",
      "Station: \"Centre St & Chambers St\"  Importance: 3.15384598074596   \n",
      "Station: \"Broadway & E 14 St\"  Importance: 3.0325702821942904   \n",
      "Station: \"12 Ave & W 40 St\"  Importance: 2.9542359558471127   \n",
      "Station: \"W 20 St & 11 Ave\"  Importance: 2.8726816010849965   \n"
     ]
    }
   ],
   "source": [
    "//Print the output here\n",
    "inTop10.sortBy(_._2,ascending=false).map(b=>s\"Station: ${b._1}  Importance: ${b._2}   \").collect().take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-22 23:49:29 WARN  BlockManager:66 - Asked to remove block rdd_1819_5, which does not exist\n",
      "2019-04-22 23:49:29 WARN  BlockManager:66 - Asked to remove block rdd_1819_6, which does not exist\n",
      "2019-04-22 23:49:29 WARN  BlockManager:66 - Asked to remove block rdd_1819_11, which does not exist\n",
      "2019-04-22 23:49:29 WARN  BlockManager:66 - Asked to remove block rdd_1819_17, which does not exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "edge: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Int]] = MapPartitionsRDD[993] at map at <console>:40\r\n",
       "new_graph: org.apache.spark.graphx.Graph[String,Int] = org.apache.spark.graphx.impl.GraphImpl@1b881bdf\r\n",
       "rank: org.apache.spark.graphx.VertexRDD[Double] = VertexRDDImpl[1846] at RDD at VertexRDD.scala:57\r\n",
       "o_rank: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, Double)] = ParallelCollectionRDD[1858] at parallelize at <console>:43\r\n",
       "station: org.apache.spark.graphx.VertexRDD[String] = VertexRDDImpl[16] at RDD at VertexRDD.scala:57\r\n",
       "outTop10: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[1862] at map at <console>:45\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val edge = graph.edges.map(e=>Edge(e.dstId,e.srcId,e.attr))\n",
    "val new_graph = Graph(graph.vertices,edge)\n",
    "val rank = new_graph.pageRank(0.0001).vertices\n",
    "val o_rank = sc.parallelize(rank.sortBy(_._2,ascending=false).take(10))\n",
    "val station = graph.vertices\n",
    "val outTop10 = station.join(o_rank).map{\n",
    "    case (a,(b,c)) => (b,c)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station: \"Hs Don't Use\"  Importance: 5.710640869520747   \n",
      "Station: \"Pershing Square North\"  Importance: 5.012823444592195   \n",
      "Station: \"WS Don't Use\"  Importance: 4.272284643284593   \n",
      "Station: \"Broadway & E 22 St\"  Importance: 3.4515211069038183   \n",
      "Station: \"E 17 St & Broadway\"  Importance: 3.3347259745457443   \n",
      "Station: \"W 21 St & 6 Ave\"  Importance: 3.2837275306670697   \n",
      "Station: \"Grand Army Plaza & Central Park S\"  Importance: 3.269839753653884   \n",
      "Station: \"West St & Chambers St\"  Importance: 3.2398990476596197   \n",
      "Station: \"8 Ave & W 31 St\"  Importance: 3.2023851500938383   \n",
      "Station: \"Central Park S & 6 Ave\"  Importance: 3.0487493283620557   \n"
     ]
    }
   ],
   "source": [
    "//Print the output here\n",
    "outTop10.sortBy(_._2,ascending=false).map(b=>s\"Station: ${b._1}  Importance: ${b._2}   \").collect().take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
