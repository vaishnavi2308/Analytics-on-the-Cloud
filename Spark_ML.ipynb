{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.14:4042\n",
       "SparkContext available as 'sc' (version = 2.4.0, master = local[*], app id = local-1554776823802)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string ... 7 more fields]\r\n",
       "training_df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [_c0: string, _c1: string ... 7 more fields]\r\n",
       "testing_df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [_c0: string, _c1: string ... 7 more fields]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.format(\"csv\").option(\"header\", \"false\").load(\"C://Users//vaish//Downloads//cal_housing (1).data\")\n",
    "val Array(training_df,testing_df) = df.randomSplit(Array(0.8,0.2),seed=1234L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\r\n",
       "import org.apache.spark.ml.feature.VectorAssembler\r\n",
       "import org.apache.spark.ml.linalg.{Matrix, Vectors}\r\n",
       "import org.apache.spark.ml.feature.StandardScaler\r\n",
       "import org.apache.spark.ml.regression.LinearRegression\r\n",
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\r\n",
       "import org.apache.spark.sql.DataFrame\r\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\r\n",
       "prepareData: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.linalg.{Matrix, Vectors}\n",
    "import org.apache.spark.ml.feature.StandardScaler\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.sql.DataFrame//so that spark knows the Dataframe type, without this, spark won't know implicit type for input\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "\n",
    "\n",
    "def prepareData(df: DataFrame): DataFrame = {\n",
    "    val new_df = df.toDF(\"Longitude\",\"Latitude\",\"MedianAge\",\"TotalRooms\",\"TotalBedrooms\",\"Population\",\"Households\",\"MedianIncome\",\"MedianHomeValue\")\n",
    "    val df2 = new_df.schema.fields.foldLeft(new_df){ \n",
    "      (df, s) => df.withColumn(s.name, df(s.name).cast(FloatType))     \n",
    "    }\n",
    "    val df1 = df2.withColumn(\"MedianHomeValue\",$\"MedianHomeValue\"/100000)\n",
    "    val df_with_features = df1.withColumn(\"RoomsPerHouse\", col(\"TotalRooms\")/col(\"Households\"))\n",
    "       .withColumn(\"PeoplePerHouse\", col(\"Population\")/col(\"Households\"))\n",
    "       .withColumn(\"BedroomsPerHouse\", col(\"TotalBedrooms\")/col(\"Households\"))\n",
    "    val df_analysis = df_with_features.select(\"MedianHomeValue\",\"MedianAge\",\"TotalRooms\",\n",
    "                  \"TotalBedRooms\", \n",
    "                  \"Population\", \n",
    "                  \"Households\", \n",
    "                  \"MedianIncome\", \n",
    "                  \"RoomsPerHouse\", \n",
    "                  \"PeoplePerHouse\", \n",
    "                  \"BedroomsPerHouse\")\n",
    "    df_analysis\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//val new_df = prepareData(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//new_df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.QuantileDiscretizer\r\n",
       "import org.apache.spark.ml.feature.Bucketizer\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//import org.apache.spark.ml.feature\n",
    "//import org.apache.spark.ml.PipelineStage\n",
    "//import org.apache.spark.ml.Estimator\n",
    "import org.apache.spark.ml.feature.QuantileDiscretizer\n",
    "//import org.apache.spark.ml.feature.Bucketizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "discretizer: org.apache.spark.ml.feature.QuantileDiscretizer = quantileDiscretizer_b21b05ff3bf2\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val discretizer = new QuantileDiscretizer().setNumBuckets(10).setInputCols(Array(\"MedianAge\",\n",
    "                  \"TotalRooms\",\"TotalBedRooms\", \"Population\", \"Households\", \"MedianIncome\", \"RoomsPerHouse\", \"PeoplePerHouse\", \n",
    "                  \"BedroomsPerHouse\")).setOutputCols(Array(\"MedianAge1\",\"TotalRooms1\",\"TotalBedRooms1\", \"Population1\", \"Households1\", \n",
    "                  \"MedianIncome1\", \"RoomsPerHouse1\", \"PeoplePerHouse1\", \"BedroomsPerHouse1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//val result = discretizer.fit(new_df).transform(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//result.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//val Array(train,test) = result.randomSplit(Array(0.8,0.2),seed=1234L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler1: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_f0ffdfe7cbfd\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler1 = new VectorAssembler()\n",
    "    .setInputCols(Array(\"MedianAge1\",\"TotalRooms1\",\"TotalBedRooms1\", \"Population1\", \"Households1\", \n",
    "                        \"MedianIncome1\", \"RoomsPerHouse1\", \"PeoplePerHouse1\", \"BedroomsPerHouse1\"))\n",
    "    .setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.regression.LinearRegression = linReg_75c5e74f0ddd\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lr  = new LinearRegression()\n",
    "    .setMaxIter(10)\n",
    "    .setRegParam(0.3)\n",
    "    .setElasticNetParam(0.8)\n",
    "    .setFeaturesCol(\"features\")\n",
    "    .setLabelCol(\"MedianHomeValue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_ff90a8ac9567\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline = new Pipeline().setStages(Array(discretizer, assembler1, lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-08 22:27:35 WARN  BLAS:61 - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "2019-04-08 22:27:35 WARN  BLAS:61 - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.PipelineModel = pipeline_ff90a8ac9567\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = pipeline.fit(prepareData(training_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "testing_predictions: org.apache.spark.sql.DataFrame = [MedianHomeValue: double, MedianAge: float ... 19 more fields]\r\n",
       "training_predictions: org.apache.spark.sql.DataFrame = [MedianHomeValue: double, MedianAge: float ... 19 more fields]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val testing_predictions = model.transform(prepareData(testing_df))\n",
    "val training_predictions = model.transform(prepareData(training_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_6b3cfd482b64\r\n",
       "rmse_train: Double = 0.9098969671559932\r\n",
       "rmse_test: Double = 0.9193915412322796\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new RegressionEvaluator()\n",
    "  .setLabelCol(\"MedianHomeValue\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"rmse\")\n",
    "val rmse_train = evaluator.evaluate(training_predictions)//Training RMSE\n",
    "val rmse_test = evaluator.evaluate(testing_predictions)//Testing RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>so the rmse for training dataset is  0.9098969671559932, the rmse for testing dataset is 0.9193915412322796</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> model intercept and coefficients </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.regression.LinearRegressionModel\r\n",
       "lrModel: org.apache.spark.ml.regression.LinearRegressionModel = linReg_75c5e74f0ddd\r\n",
       "res0: org.apache.spark.ml.linalg.Vector = [0.0,0.0,0.0,0.0,0.0,0.16306051560031795,0.0,-0.019166800065469336,0.0]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.regression.LinearRegressionModel\n",
    "val lrModel = model.stages(2).asInstanceOf[LinearRegressionModel]\n",
    "lrModel.coefficients//model coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: org.apache.spark.ml.Transformer = linReg_75c5e74f0ddd\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.stages(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Double = 1.4172017005389426\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrModel.intercept//model uintercept"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
